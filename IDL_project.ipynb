{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IDL_project.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "-0e39G18Aqnk"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eh7hyRTugU7A"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\", force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-Levenshtein\n",
        "!git clone --recursive https://github.com/parlance/ctcdecode.git\n",
        "!pip install wget\n",
        "%cd ctcdecode\n",
        "!pip install .\n",
        "%cd ..\n",
        "\n",
        "!pip install torchsummaryX # We also install a summary package to check our model's forward before training"
      ],
      "metadata": {
        "id": "-JjudS3KhF5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DataLoader"
      ],
      "metadata": {
        "id": "lS5aRLs-gr2B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_file_paths(data_dirs, reskin_file = \"reskin_data.csv\" ):\n",
        "    \"\"\"\n",
        "    Input: is a ratio of training to validation split. They should sum up to 100 and paths to directories.\n",
        "    Output: Paths to maintain this split across each class (The main idea being to approximately\n",
        "    maintain the same ratio. 0 will still be dominant but rest of the classes should be equal in datapoints roughly )\n",
        "    \"\"\"\n",
        "    if(type(data_dirs) == str):\n",
        "        data_dirs = [data_dirs]\n",
        "    reskin_paths = {\"-1cloth\":[],\"0cloth\":[], \"1cloth\":[], \"2cloth\": [], \"3cloth\": []}\n",
        "    for data_dir in data_dirs:\n",
        "        class_dirs = os.listdir(data_dir)\n",
        "        for class_dir in class_dirs:\n",
        "            temp = []\n",
        "            path_dirs = os.listdir(data_dir + \"/\" + class_dir)\n",
        "            for path_dir in path_dirs:\n",
        "                reskin_file_path = data_dir + \"/\" + class_dir + \"/\" + path_dir + \"/\" + reskin_file\n",
        "                if(\"0cloth\" in class_dir):\n",
        "                    reskin_paths[\"0cloth\"].append([reskin_file_path, 0])\n",
        "                elif(\"1cloth\" in class_dir):\n",
        "                    reskin_paths[\"1cloth\"].append([reskin_file_path, 1])\n",
        "                elif(\"2cloth\" in class_dir):\n",
        "                    reskin_paths[\"2cloth\"].append([reskin_file_path, 2])\n",
        "                elif(\"3cloth\" in class_dir):\n",
        "                    reskin_paths[\"3cloth\"].append([reskin_file_path, 3])\n",
        "    return reskin_paths\n",
        "\n",
        "\n",
        "def setup_paths(data_dirs, train_val_test_split=[0.7, 0.2, 0.1]):\n",
        "    paths = get_file_paths(data_dirs)\n",
        "    train_paths = []\n",
        "    val_paths = []\n",
        "    test_paths = []\n",
        "    for key in paths.keys():\n",
        "      # if(self.shuffle):\n",
        "      #     random.shuffle(paths[key])\n",
        "      # else:\n",
        "      #     pass\n",
        "      train_num = int(train_val_test_split[0]*len(paths[key]))\n",
        "      val_num = int(train_val_test_split[1]*len(paths[key]))\n",
        "      train_paths+=paths[key][:train_num]\n",
        "      val_paths+=paths[key][train_num:train_num+val_num]\n",
        "      test_paths+=paths[key][train_num+val_num:]\n",
        "    \n",
        "    return train_paths, val_paths, test_paths"
      ],
      "metadata": {
        "id": "iyXj_uA-XDEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TestData loader"
      ],
      "metadata": {
        "id": "-0e39G18Aqnk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "dirn = \"/content/drive/MyDrive/idl_project/Basic_Dataset_RealTrials_NoRub\"\n",
        "train_paths, val_paths, test_paths = setup_paths(dirn)\n",
        "\n",
        "print(train_paths)\n",
        "print(val_paths)\n",
        "print(test_paths)\n"
      ],
      "metadata": {
        "id": "SSDsOYAOAmtS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#LSTM model"
      ],
      "metadata": {
        "id": "U1L1yQrVA1LF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchsummaryX import summary\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "import gc\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import datetime\n",
        "import time\n",
        "# imports for decoding and distance calculation\n",
        "import ctcdecode\n",
        "import Levenshtein\n",
        "from ctcdecode import CTCBeamDecoder\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Device: \", device)"
      ],
      "metadata": {
        "id": "uIF7QoZEBS_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LibriSamples(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__ (self, data_path):  # You can use partition to specify train or dev\n",
        "\n",
        "        self.X = []\n",
        "        self.Y = []\n",
        "\n",
        "        for path, label in data_path:  # path [file_location, label]\n",
        "            data = np.loadtxt(path, delimiter=\",\")\n",
        "            data = np.float32((data[:,:15]))\n",
        "            self.X.append(data)\n",
        "            labels = np.empty(5)\n",
        "            labels.fill(label)\n",
        "            self.Y.append(labels)\n",
        "\n",
        "        assert (len(self.X) == len(self.Y))\n",
        "\n",
        "    def __len__ (self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__ (self, idx):\n",
        "        X = self.X[idx]\n",
        "        Y = self.Y[idx]\n",
        "\n",
        "        X = torch.tensor(X)\n",
        "        Y = torch.tensor(Y)\n",
        "\n",
        "        return X, Y\n",
        "\n",
        "    def collate_fn (self, batch):\n",
        "        # print(\"call collate_fn in Library samples batch: \", type(batch), np.array(batch).shape, np.array(batch[0][0]).shape, np.array(batch[0][1]).shape)\n",
        "        batch_x = [x for x, y in batch]\n",
        "        batch_y = [y for x, y in batch]\n",
        "\n",
        "        # print(\"batch_x batch_y\", np.array(batch_x[0]).shape, np.array(batch_y[0]).shape)\n",
        "        batch_x_pad = pad_sequence(batch_x,\n",
        "                                   batch_first=True)  # TODO: pad the sequence with pad_sequence (already imported)\n",
        "        lengths_x = [x.shape[0] for x in batch_x]  # TODO: Get original lengths of the sequence before padding\n",
        "        batch_y_pad = pad_sequence(batch_y,\n",
        "                                   batch_first=True)  # TODO: pad the sequence with pad_sequence (already imported)\n",
        "        lengths_y = [y.shape[0] for y in batch_y]  # TODO: Get original lengths of the sequence before padding\n",
        "\n",
        "        # print(lengths_x, len(lengths_x))\n",
        "        # print(lengths_y)\n",
        "\n",
        "        return batch_x_pad, batch_y_pad, torch.tensor(lengths_x), torch.tensor(lengths_y)\n"
      ],
      "metadata": {
        "id": "nfNWFFoiDIdQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 10\n",
        "\n",
        "root = \"/content/drive/MyDrive/idl_project/Basic_Dataset_RealTrials_NoRub\"\n",
        "train_paths, val_paths, test_paths = setup_paths(root)\n",
        "\n",
        "train_data = LibriSamples(train_paths)\n",
        "val_data = LibriSamples(val_paths)\n",
        "test_data = LibriSamples(test_paths)\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, collate_fn=train_data.collate_fn, shuffle=False, drop_last=False, num_workers=2)# TODO: Define the train loader. Remember to pass in a parameter (function) for the collate_fn argument \n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, collate_fn=val_data.collate_fn, shuffle=False, drop_last=False, num_workers=1)# TODO: Define the val loader. Remember to pass in a parameter (function) for the collate_fn argument \n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, collate_fn=test_data.collate_fn, shuffle=False, drop_last=False, num_workers=2)# TODO: Define the test loader. Remember to pass in a parameter (function) for the collate_fn argument \n",
        "\n",
        "print(\"Batch size: \", batch_size)\n",
        "print(\"Train dataset samples = {}, batches = {}\".format(train_data.__len__(), len(train_loader)))\n",
        "print(\"Val dataset samples = {}, batches = {}\".format(val_data.__len__(), len(val_loader)))\n",
        "print(\"Test dataset samples = {}, batches = {}\".format(test_data.__len__(), len(test_loader)))"
      ],
      "metadata": {
        "id": "oqwPXQyLbqL2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional\n",
        "# Test code for checking shapes and return arguments of the train and val loaders\n",
        "for data in test_loader: # data shape(Time, Batch, feature)\n",
        "    x, y, lx, ly = data # if you face an error saying \"Cannot unpack\", then you are not passing the collate_fn argument\n",
        "    print(x.shape, y.shape, lx.shape, ly.shape)\n",
        "    # print(x)\n",
        "    # print(y[0].shape)\n",
        "    break"
      ],
      "metadata": {
        "id": "iMGg2e_nA_TQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Network(nn.Module):\n",
        "\n",
        "    def __init__(self): # You can add any extra arguments as you wish\n",
        "\n",
        "        super(Network, self).__init__()\n",
        "\n",
        "        # Embedding layer converts the raw input into features which may (or may not) help the LSTM to learn better \n",
        "        # For the very low cut-off you dont require an embedding layer. You can pass the input directly to the  LSTM\n",
        "        # self.embedding = \n",
        "        \n",
        "        self.lstm = nn.LSTM(15, 256, 1, batch_first=True)# TODO: # Create a single layer, uni-directional LSTM with hidden_size = 256\n",
        "        # Use nn.LSTM() Make sure that you give in the proper arguments as given in https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\n",
        "\n",
        "        self.classification = nn.Linear(256, 3)# TODO: Create a single classification layer using nn.Linear()\n",
        "\n",
        "    def forward(self, x, x_origin_len): #x shape (T, B, 13)# TODO: You need to pass atleast 1 more parameter apart from self and x\n",
        "        # print(\"------Start of LSTM forward------\")\n",
        "        # print(\"call forward\")\n",
        "        # print(x.size(), x_origin_len.size())\n",
        "        # print(x)\n",
        "        # print(x_origin_len)\n",
        "        # x is returned from the dataloader. So it is assumed to be padded with the help of the collate_fn\n",
        "        # Because Batch_first is true, so the input for pack should be (B, T, 13)\n",
        "        # since in pad_sequence we chose batch_first, so the input already be (B, T, 13)\n",
        "        x_lstm_in = x\n",
        "        # print(\"LSTM in (B,T,13)\",x_lstm_in.size())\n",
        "        packed_input = pack_padded_sequence(x_lstm_in, x_origin_len, enforce_sorted=False, batch_first=True)# TODO: Pack the input with pack_padded_sequence. Look at the parameters it requires\n",
        "\n",
        "        # out1 (B, T, 256) because batch_first is true\n",
        "        out1, (out2, out3) = self.lstm(packed_input)# TODO: Pass packed input to self.lstm\n",
        "        # As you may see from the LSTM docs, LSTM returns 3 vectors. Which one do you need to pass to the next function?\n",
        "        out, lengths  = pad_packed_sequence(out1, batch_first=True)# TODO: Need to 'unpack' the LSTM output using pad_packed_sequence\n",
        "        # print(\"unpacked LSTM output (B,T,256) \", out.size())\n",
        "        out = self.classification(out)# TODO: Pass unpacked LSTM output to the classification layer\n",
        "        # print(\"Linear cls out shape (B,T,41)\",out.size())\n",
        "        # out = # Optional: Do log softmax on the output. Which dimension?\n",
        "        log_soft_max = nn.LogSoftmax(2)\n",
        "        out_prob = log_soft_max(out)\n",
        "        # print(\"out_prob shape (B,T,41)\",out_prob.size())\n",
        "        # print(\"------End of LSTM forward------\")\n",
        "        # print(out[0][0], out_prob[0][0])\n",
        "        return out_prob, lengths # TODO: Need to return 2 variables\n",
        "\n",
        "model = Network().to(device)\n",
        "print(model)\n",
        "summary(model, x.to(device), lx) # x and lx are from the previous cell"
      ],
      "metadata": {
        "id": "D8pWBwJIA4Ul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Configuration"
      ],
      "metadata": {
        "id": "OQMofM-_lRxg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.cuda()\n",
        "\n",
        "epochs = 20\n",
        "\n",
        "criterion = nn.CTCLoss()# TODO: What loss do you need for sequence to sequence models? \n",
        "# Do you need to transpose or permute the model output to find out the loss? Read its documentation\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=2e-3)# TODO: Adam works well with LSTM (use lr = 2e-3)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5, patience=1, verbose=True)\n",
        "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=(len(train_loader) * epochs))\n",
        "# decoder = CTCBeamDecoder(\n",
        "#     labels=PHONEME_MAP,\n",
        "#     model_path=None,\n",
        "#     alpha=0,\n",
        "#     beta=0,\n",
        "#     cutoff_top_n=40,\n",
        "#     cutoff_prob=1.0,\n",
        "#     beam_width=10,\n",
        "#     num_processes=4,\n",
        "#     blank_id=0,\n",
        "#     log_probs_input=True\n",
        "# )\n",
        "scaler = torch.cuda.amp.GradScaler()"
      ],
      "metadata": {
        "id": "D3oa5TrhlL01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Trian and evaluate"
      ],
      "metadata": {
        "id": "9IbHgCTPl7tl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model_evaluate(model, device, dev_samples):\n",
        "    \n",
        "  model.eval()\n",
        "  batch_bar = tqdm(total=len(dev_samples), dynamic_ncols=True, position=0, leave=False, desc='Val')\n",
        "  avg_dist = 0\n",
        "  total_loss = 0.0\n",
        "  for i, (x, y, lx, ly) in enumerate(dev_samples):\n",
        "      x = x.cuda()\n",
        "      y = y.cuda()\n",
        "\n",
        "      with torch.no_grad():\n",
        "          output, length = model(x, lx)\n",
        "          lost_input = output.permute(1, 0, 2) # (T,B,41)\n",
        "          loss = criterion(lost_input, y, length, ly)\n",
        "          total_loss += loss.item()\n",
        "\n",
        "      # dist = calculate_levenshtein(output, y, length, ly, decoder, PHONEME_MAP)\n",
        "      avg_dist += 0\n",
        "      batch_bar.set_postfix(distance=\"{:.04f}\".format(avg_dist))\n",
        "\n",
        "  avg_dist = avg_dist / len(dev_samples) \n",
        "  avg_loss = total_loss / len(dev_samples)    \n",
        "  batch_bar.close()\n",
        "  # print(\"Batch avg dist: {:.04f}\".format(avg_dist))\n",
        "  \n",
        "  return avg_loss, avg_dist\n"
      ],
      "metadata": {
        "id": "CtHZUul7l7Hb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n",
        "\n",
        "# TODO: Write the model training code \n",
        "\n",
        "# You are free to write your own code for training or you can use the code from previous homeworks' starter notebooks\n",
        "# However, you will have to make modifications because of the following.\n",
        "# (1) The dataloader returns 4 items unlike 2 for hw2p2\n",
        "# (2) The model forward returns 2 outputs\n",
        "# (3) The loss may require transpose or permuting\n",
        "\n",
        "# Tip: Implement mixed precision training\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "distance = 0\n",
        "for epoch in range(epochs):\n",
        "    # Quality of life tip: leave=False and position=0 are needed to make tqdm usable in jupyter\n",
        "    batch_bar = tqdm(total=len(train_loader), dynamic_ncols=True, leave=False, position=0, desc='Train') \n",
        "\n",
        "    total_loss = 0\n",
        "\n",
        "    model.train()\n",
        "    for i, (x, y, lx, ly) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        x = x.cuda()\n",
        "        y = y.cuda()\n",
        "\n",
        "        # Don't be surprised - we just wrap these two lines to make it work for FP16\n",
        "        with torch.cuda.amp.autocast():     \n",
        "            output, length = model(x, lx)\n",
        "            # print(\"Model out shape(B, T, 41), output shape (B, T)\", output.size(), length.size())\n",
        "            lost_input = output.permute(1, 0, 2) # (T,B,41)\n",
        "            loss = criterion(lost_input, y, length, ly)\n",
        "\n",
        "        # Update # correct & loss as we go\n",
        "        total_loss += float(loss)\n",
        "\n",
        "        # tqdm lets you add some details so you can monitor training as you train.\n",
        "        batch_bar.set_postfix(\n",
        "            epoch=\"{}\".format(epoch),\n",
        "            distance=\"{:.04f}%\".format(distance),\n",
        "            loss=\"{:.04f}\".format(float(total_loss / (i + 1))),\n",
        "            lr=\"{:.04f}\".format(float(optimizer.param_groups[0]['lr'])))\n",
        "        \n",
        "        # Another couple things you need for FP16. \n",
        "        scaler.scale(loss).backward() # This is a replacement for loss.backward()\n",
        "        scaler.step(optimizer) # This is a replacement for optimizer.step()\n",
        "        scaler.update() # This is something added just for FP16\n",
        "\n",
        "        batch_bar.update() # Update tqdm bar\n",
        "\n",
        "    batch_bar.close() # You need this to close the tqdm bar\n",
        "\n",
        "    # You can add validation per-epoch here if you would like\n",
        "    avg_loss, distance = model_evaluate(model, device, val_loader)\n",
        "    scheduler.step(avg_loss) \n",
        "    # Save model\n",
        "    # train_acc = 100 * num_correct / (len(train_loader) * batch_size)\n",
        "    # stats = {\n",
        "    #   \"epoch\":epoch,\n",
        "    #   \"train_stats\": 0,\n",
        "    #   \"eval_stats\": 0,\n",
        "    #   \"lr\": optimizer.param_groups[0][\"lr\"]\n",
        "    # }\n",
        "\n",
        "    # # model_saver.save(StoredModel(model, optimizer, scheduler, criterion), stats, train_acc)\n",
        "\n",
        "    print(\"Epoch {}/{}: Distance {}, Train Loss {:.04f}, Learning Rate {:.04f}\".format(\n",
        "        epoch + 1,\n",
        "        epochs,\n",
        "        distance,\n",
        "        float(total_loss / len(train_loader)),\n",
        "        float(optimizer.param_groups[0]['lr'])))\n",
        "\n",
        "model_path = \"/content/drive/MyDrive/idl_project/trained_model\"\n",
        "model_name = \"model_LSTM_\" + time.strftime(\"%Y_%m_%d_%H_%M_%S\", time.localtime())\n",
        "torch.save(model.state_dict(),model_path + model_name)"
      ],
      "metadata": {
        "id": "Ds_7WisSmCpe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
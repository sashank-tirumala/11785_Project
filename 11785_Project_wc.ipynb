{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MrUKiCENr9Ah",
        "outputId": "9473003c-c5e5-4be6-ae34-093d2142a649"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\", force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_1QUJQrZaLZv"
      },
      "outputs": [],
      "source": [
        "# ToDo: Need to change folder to your own path \n",
        "!unzip -q -o /content/drive/MyDrive/idl_project/Large_Dataset.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GQ4BkuK3ZgyG"
      },
      "outputs": [],
      "source": [
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AyVC9HIRbx1Q",
        "outputId": "ef878a1f-4e34-4c14-d1f5-e2823c76589f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        }
      ],
      "source": [
        "!wandb login"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install seaborn"
      ],
      "metadata": {
        "id": "4O0SqVvScHQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HMpc8S6vamly"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pdb\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as ttf\n",
        "\n",
        "import os\n",
        "import os.path as osp\n",
        "import os\n",
        "from typing import List\n",
        "from typing import Optional\n",
        "\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import sklearn\n",
        "import numpy as np\n",
        "import wandb\n",
        "\n",
        "import logging\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "import seaborn\n",
        "\n",
        "ROOT_DIR = '/content/Large_Dataset'\n",
        "\n",
        "\n",
        "def get_file_paths(data_dirs, reskin_file = \"reskin_data.csv\" ):\n",
        "    \"\"\"\n",
        "    Input: is a ratio of training to validation split. They should sum up to 100 and paths to directories.\n",
        "    Output: Paths to maintain this split across each class (The main idea being to approximately\n",
        "    maintain the same ratio. 0 will still be dominant but rest of the classes should be equal in datapoints roughly )\n",
        "    \"\"\"\n",
        "    if(type(data_dirs) == str):\n",
        "        data_dirs = [data_dirs]\n",
        "    reskin_paths = {\"-1cloth\":[],\"0cloth\":[], \"1cloth\":[], \"2cloth\": [], \"3cloth\": []}\n",
        "    for data_dir in data_dirs:\n",
        "        class_dirs = os.listdir(data_dir)\n",
        "        for class_dir in class_dirs:\n",
        "            temp = []\n",
        "            path_dirs = os.listdir(data_dir + \"/\" + class_dir)\n",
        "            for path_dir in path_dirs:\n",
        "                reskin_file_path = data_dir + \"/\" + class_dir + \"/\" + path_dir + \"/\" + reskin_file\n",
        "                if(\"0cloth\" in class_dir):\n",
        "                    reskin_paths[\"0cloth\"].append(reskin_file_path)\n",
        "                elif(\"1cloth\" in class_dir):\n",
        "                    reskin_paths[\"1cloth\"].append(reskin_file_path)\n",
        "                elif(\"2cloth\" in class_dir):\n",
        "                    reskin_paths[\"2cloth\"].append(reskin_file_path)\n",
        "                elif(\"3cloth\" in class_dir):\n",
        "                    reskin_paths[\"3cloth\"].append(reskin_file_path)\n",
        "    return reskin_paths\n",
        "\n",
        "def get_context(data, context, offset=-1, time_idx = -1, class_idx = -2, include_transition = False, context_type = \"double\"):\n",
        "    \"\"\"\n",
        "    Input: 2D np array -> raw data from reskin csv file\n",
        "    Output: dictionary with class followed context values\n",
        "    \"\"\"\n",
        "    res = []\n",
        "    start_idx = context\n",
        "    if(context_type == \"double\"):\n",
        "        end_idx = data.shape[0] - context - 1\n",
        "    elif(context_type == \"left\"):\n",
        "        end_idx =  data.shape[0]\n",
        "    else:\n",
        "        print(\"Invalid context type\")\n",
        "        return None\n",
        "    for i in np.arange(start_idx, end_idx, 1):\n",
        "        \n",
        "        if(context_type == \"double\"):\n",
        "            cur_val = np.array(data[i-context: i+context+1,:])\n",
        "        elif(context_type == \"left\"):\n",
        "            cur_val = np.array(data[i-context:i,:])\n",
        "        else:\n",
        "            print(\"Invalid context type\")\n",
        "            return None\n",
        "        bool_arr = cur_val[:,class_idx] == cur_val[0,class_idx]\n",
        "        if(np.all(bool_arr) or include_transition): #Checks if the value is in a transition or not --> For now removing, I can disable this \n",
        "            label = int(cur_val[0,class_idx])+offset\n",
        "            res.append([np.delete(cur_val, [time_idx, class_idx], axis=1), label])\n",
        "    return res\n",
        "\n",
        "def std_normalizer(data):\n",
        "    x_vals = []\n",
        "    for dat in data:\n",
        "        x_vals.append(dat[0])\n",
        "    x_vals = np.hstack(x_vals)\n",
        "    x_mean  = np.mean(x_vals, axis=0).reshape(1,-1)\n",
        "    x_std = np.std(x_vals, axis=0).reshape(1,-1)\n",
        "    for dat in data:\n",
        "        res_x = np.zeros(dat[0].shape)\n",
        "        for col in range(dat[0].shape[1]):\n",
        "            res_x[:,col] = dat[0][:,col] - x_mean[0, col]\n",
        "            res_x[:,col] = res_x[:,col] /(x_std[0, col]+1e-6)\n",
        "        dat[0] = res_x\n",
        "    return data\n",
        "\n",
        "def get_data(paths):\n",
        "    data = []\n",
        "    for path in paths:\n",
        "        temp = np.loadtxt(path, delimiter=\",\")\n",
        "        #pdb.set_trace()\n",
        "        data.append(temp)\n",
        "        #print('S',np.shape(temp))\n",
        "    #pdb.set_trace()\n",
        "    return data\n",
        "\n",
        "def setup_paths(data_dirs, train_val_test_split=[0.7, 0.2, 0.1]):\n",
        "    paths = get_file_paths(data_dirs)\n",
        "    train_paths = []\n",
        "    val_paths = []\n",
        "    test_paths = []\n",
        "    for key in paths.keys():\n",
        "      # if(self.shuffle):\n",
        "      #     random.shuffle(paths[key])\n",
        "      # else:\n",
        "      #     pass\n",
        "      train_num = int(train_val_test_split[0]*len(paths[key]))\n",
        "      val_num = int(train_val_test_split[1]*len(paths[key]))\n",
        "      train_paths+=paths[key][:train_num]\n",
        "      val_paths+=paths[key][train_num:train_num+val_num]\n",
        "      test_paths+=paths[key][train_num+val_num:]\n",
        "    \n",
        "    return train_paths, val_paths, test_paths\n",
        "\n",
        "class ClothDataSet(Dataset):\n",
        "    def __init__(self, paths, transforms, context = 5, normalizer = std_normalizer, get_context = get_context, label_offset = -1,\n",
        "    time_idx = -1, class_idx = -2, include_transition = False, context_type = \"double\", shuffle=True):\n",
        "        self.paths = paths\n",
        "        self.transforms = transforms\n",
        "        self.normalizer = std_normalizer\n",
        "        self.get_context = get_context\n",
        "        self.label_offset = label_offset\n",
        "        self.time_idx = time_idx\n",
        "        self.class_idx = class_idx\n",
        "        self.include_transition = include_transition\n",
        "        self.context_type = context_type\n",
        "        self.data = 0\n",
        "        self.shuffle = shuffle\n",
        "        self.context = context\n",
        "        self.setup()\n",
        "\n",
        "    def setup(self):\n",
        "        \"\"\"\n",
        "        store it in an array of arrays [[ x, y], [x, y] and so] x is a 2D Array btw, \n",
        "        normalize the entire dataset (only x)\n",
        "        shuffle the data completely (shuffling within a particular dataset is fine, just not across datasets)\n",
        "        store it in some variable\n",
        "        \"\"\"\n",
        "        self.data = get_data(self.paths)\n",
        "        temp = []\n",
        "        for data in self.data:\n",
        "            temp=temp+get_context(data, self.context, self.label_offset, self.time_idx, self.class_idx, self.include_transition, self.context_type)\n",
        "        self.data = temp\n",
        "        self.data = self.normalizer(self.data)\n",
        "        if(self.shuffle):\n",
        "            random.shuffle(self.data)\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Think of what data augmentations you want to perform before finally outputting your code\n",
        "        \"\"\"\n",
        "        #TODO perform transforms\n",
        "        return self.data[idx]\n",
        "\n",
        "\n",
        "if(__name__ == \"__main__\"):\n",
        "    dirn = str(ROOT_DIR) \n",
        "    res = get_file_paths(dirn)\n",
        "    paths = res[\"0cloth\"]\n",
        "    ## \"0cloth\" , \"1cloth \" , \"2cloth\" , \"3cloth\"\n",
        "    #print(paths)\n",
        "    # dat = get_data(res[\"0cloth\"])\n",
        "    # temp = []\n",
        "    # for data in dat:\n",
        "    #     temp=temp+get_context(data, 5)\n",
        "    # data = temp\n",
        "    # data = std_normalizer(data)\n",
        "    data = ClothDataSet(paths, None)\n",
        "    train_loader = DataLoader(data, batch_size=128,\n",
        "                          shuffle=True, drop_last=True, num_workers=2)\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nj4xdRkAo9-n"
      },
      "outputs": [],
      "source": [
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "class ClothDataSet(Dataset):\n",
        "    def __init__(self, paths, transforms, context = 5, normalizer = std_normalizer, get_context = get_context, label_offset = -1,\n",
        "    time_idx = -1, class_idx = -2, include_transition = False, context_type = \"double\", shuffle=True):\n",
        "        self.paths = paths\n",
        "        self.transforms = transforms\n",
        "        self.normalizer = std_normalizer\n",
        "        self.get_context = get_context\n",
        "        self.label_offset = label_offset\n",
        "        self.time_idx = time_idx\n",
        "        self.class_idx = class_idx\n",
        "        self.include_transition = include_transition\n",
        "        self.context_type = context_type\n",
        "        self.data = 0\n",
        "        self.shuffle = shuffle\n",
        "        self.context = context\n",
        "        self.setup()\n",
        "        self.labels = [-1,0,1,2,3]\n",
        "\n",
        "\n",
        "    def setup(self):\n",
        "        \"\"\"\n",
        "        store it in an array of arrays [[ x, y], [x, y] and so] x is a 2D Array btw, \n",
        "        normalize the entire dataset (only x)\n",
        "        shuffle the data completely (shuffling within a particular dataset is fine, just not across datasets)\n",
        "        store it in some variable\n",
        "        \"\"\"\n",
        "        self.data = get_data(self.paths)\n",
        "        #temp = []\n",
        "        #for data in self.data:\n",
        "        #    temp=temp+get_context(data, self.context, self.label_offset, self.time_idx, self.class_idx, self.include_transition, self.context_type)\n",
        "        #self.data = temp\n",
        "        #self.data = self.normalizer(self.data)\n",
        "        #if(self.shuffle):\n",
        "        #    random.shuffle(self.data)\n",
        "    def __len__(self):\n",
        "        # print(len(self.data))\n",
        "        #print('d0_s', self.data[0])\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, ind):\n",
        "        #print('ind',ind)\n",
        "        d0 = self.data[ind]\n",
        "        #print(d0.shape)\n",
        "        X_t = torch.tensor(d0[:])\n",
        "        X = torch.zeros((X_t.shape[0],15))\n",
        "        Y = torch.zeros((len(X_t),1))\n",
        "        for _ in range(len(X_t)):\n",
        "            X[_][:] = X_t[_][:15]\n",
        "            Y[_]    = self.labels.index(int(X_t[_][15] - 1)) # weichen: Need to -1 for labels\n",
        "        #print(X.shape) # TODO: Convert sequence of  phonemes into sequence of Long tensors\n",
        "        #print(Y.shape)\n",
        "\n",
        "        X = torch.tensor(X)\n",
        "        Y = torch.tensor(Y)\n",
        "\n",
        "        return X, Y\n",
        "    \n",
        "    def collate_fn(self,batch):\n",
        "\n",
        "        batch_x = [x for x,y in batch]\n",
        "        batch_y = [y for x,y in batch]\n",
        "\n",
        "        batch_x_pad = pad_sequence(batch_x,batch_first=True)# TODO: pad the sequence with pad_sequence (already imported)\n",
        "        lengths_x = [len(x) for x in batch_x]# TODO: Get original lengths of the sequence before padding\n",
        "\n",
        "        batch_y_pad = pad_sequence(batch_y, batch_first=True) # TODO: pad the sequence with pad_sequence (already imported)\n",
        "        lengths_y = [len(y) for y in batch_y]# TODO: Get original lengths of the sequence before padding\n",
        "\n",
        "        return batch_x_pad, batch_y_pad, torch.tensor(lengths_x), torch.tensor(lengths_y)\n",
        "\n",
        "\n",
        "if(__name__ == \"__main__\"):\n",
        "    dirn = str(ROOT_DIR) \n",
        "    # res = get_file_paths(dirn)\n",
        "    # paths = res[\"0cloth\"]\n",
        "    ## \"0cloth\" , \"1cloth \" , \"2cloth\" , \"3cloth\"\n",
        "    #print(paths)\n",
        "    # dat = get_data(res[\"0cloth\"])\n",
        "    # temp = []\n",
        "    # for data in dat:\n",
        "    #     temp=temp+get_context(data, 5)\n",
        "    # data = temp\n",
        "    # data = std_normalizer(data)\n",
        "    # data = ClothDataSet(paths, None)\n",
        "    # train_loader = DataLoader(data, batch_size=128,collate_fn=data.collate_fn,\n",
        "    #                       shuffle=True, drop_last=True, num_workers=2)\n",
        "\n",
        "\n",
        "    batch_size = 128\n",
        "    train_paths, val_paths, test_paths = setup_paths(dirn, train_val_test_split=[0.7, 0.2, 0.1])\n",
        "\n",
        "    train_data = ClothDataSet(train_paths, None)\n",
        "    val_data = ClothDataSet(val_paths, None)\n",
        "    test_data = ClothDataSet(test_paths, None)\n",
        "\n",
        "    train_loader = DataLoader(train_data, batch_size=batch_size, collate_fn=train_data.collate_fn, shuffle=True, drop_last=False, num_workers=2)# TODO: Define the train loader. Remember to pass in a parameter (function) for the collate_fn argument \n",
        "    val_loader = DataLoader(val_data, batch_size=batch_size, collate_fn=val_data.collate_fn, shuffle=True, drop_last=False, num_workers=1)# TODO: Define the val loader. Remember to pass in a parameter (function) for the collate_fn argument \n",
        "    test_loader = DataLoader(test_data, batch_size=batch_size, collate_fn=test_data.collate_fn, shuffle=False, drop_last=False, num_workers=2)# TODO: Define the test loader. Remember to pass in a parameter (function) for the collate_fn argument \n",
        "\n",
        "    print(\"Batch size: \", batch_size)\n",
        "    print(\"Train dataset samples = {}, batches = {}\".format(train_data.__len__(), len(train_loader)))\n",
        "    print(\"Val dataset samples = {}, batches = {}\".format(val_data.__len__(), len(val_loader)))\n",
        "    print(\"Test dataset samples = {}, batches = {}\".format(test_data.__len__(), len(test_loader)))\n",
        "\n",
        "    print(train_paths)\n",
        "    print(val_paths)\n",
        "    print(test_paths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fWCHIaShsCtb"
      },
      "outputs": [],
      "source": [
        "for data in train_loader:\n",
        "    x, y, lx, ly = data # if you face an error saying \"Cannot unpack\", then you are not passing the collate_fn argument\n",
        "    print(x.shape, y.shape, lx.shape, ly.shape)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jfc2ElvvA7eG"
      },
      "outputs": [],
      "source": [
        "class Network(nn.Module):\n",
        "\n",
        "    def __init__(self): # You can add any extra arguments as you wish\n",
        "\n",
        "        super(Network, self).__init__()\n",
        "\n",
        "        # Embedding layer converts the raw input into features which may (or may not) help the LSTM to learn better \n",
        "        # For the very low cut-off you dont require an embedding layer. You can pass the input directly to the  LSTM\n",
        "        embedding_layers = \\\n",
        "                  [\n",
        "                   nn.Conv1d(in_channels=15, out_channels=128, kernel_size=1, stride=1, padding=1),\n",
        "#                    nn.Conv1d(in_channels=13, out_channels=128, kernel_size=1),\n",
        "                   nn.BatchNorm1d(128),\n",
        "                   nn.ReLU(),\n",
        "                   nn.Dropout(p=0.3),\n",
        "                   nn.Conv1d(in_channels=128, out_channels=256, kernel_size=1, stride=1, padding=1),\n",
        "#                    nn.Conv1d(in_channels=128, out_channels=256, kernel_size=1),\n",
        "                   nn.BatchNorm1d(256),\n",
        "                   nn.ReLU(),\n",
        "                   nn.Dropout(p=0.3)\n",
        "                  ]\n",
        "              \n",
        "        self.embedding = nn.Sequential(*embedding_layers)\n",
        "  \n",
        "        # self.lstm = nn.LSTM(15,256,1) # TODO: # Create a single layer, uni-directional LSTM with hidden_size = 256\n",
        "        # self.lstm = nn.LSTM(15, 512, 1, batch_first=True, bidirectional=True, dropout=0.1)\n",
        "        self.lstm = nn.LSTM(256, 512, 1, batch_first=True, bidirectional=True, dropout=0.1)\n",
        "        # Use nn.LSTM() Make sure that you give in the proper arguments as given in https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\n",
        "        # layers = [nn.Linear(256,5)]\n",
        "        linear_layers = [nn.Linear(512*2, 2048),\n",
        "                         nn.ReLU(),\n",
        "                         nn.Dropout(p=0.3),\n",
        "                         nn.Linear(2048, 5)]\n",
        "        self.classification = nn.Sequential(*linear_layers)# TODO: Create a single classification layer using nn.Linear()\n",
        "\n",
        "        # self.classification = nn.Sequential(*layers)# TODO: Create a single classification layer using nn.Linear()\n",
        "\n",
        "    def forward(self, x,len_x): # TODO: You need to pass atleast 1 more parameter apart from self and x\n",
        "        # print(f\"x shape{x.shape} len {len_x}\")\n",
        "\n",
        "        x_cnn_in = x.permute(0, 2, 1) #(B, T, 13) -> (B, 13, T)\n",
        "        # \n",
        "        # print(f\"cnn in {x_cnn_in.shape}\")\n",
        "        x_cnn_out = self.embedding(x_cnn_in) #(B, Emb_out, T)\n",
        "        # print(f\"cnn out {x_cnn_out.shape}\")\n",
        "        x_lstm_in = x_cnn_out.permute(0, 2, 1) # (B, T, Emb_out)\n",
        "        # print(f\"lstm_in {x_lstm_in.shape}\")\n",
        "        x_origin_len = torch.clamp(len_x,max=x_lstm_in.shape[1])\n",
        "        # print(f\"x_origin_len {x_origin_len}\")\n",
        "        # x is returned from the dataloader. So it is assumed to be padded with the help of the collate_fn\n",
        "        packed_input = pack_padded_sequence(x_lstm_in, x_origin_len, enforce_sorted=False, batch_first=True)# TODO: Pack the input with pack_padded_sequence. Look at the parameters it requires\n",
        "        # print(f\"packed_input {packed_input.size()}  {x_origin_len}\")\n",
        "        out1, (out2, out3) = self.lstm(packed_input)# TODO: Pass packed input to self.lstm\n",
        "        # print(f\"packed_inout1put {out1.shape}\")\n",
        "        # As you may see from the LSTM docs, LSTM returns 3 vectors. Which one do you need to pass to the next function?\n",
        "        out, lengths  = pad_packed_sequence(out1, batch_first=True)# TODO: Need to 'unpack' the LSTM output using pad_packed_sequence\n",
        "        # print(f\"out pad {out.shape}\")\n",
        "        out = self.classification(out)# TODO: Pass unpacked LSTM output to the classification layer\n",
        "        # print(f\"out final {out.shape}\")\n",
        "        #out_l =  torch.nn.LogSoftmax(dim=2)(out)# Optional: Do log softmax on the output. Which dimension?\n",
        "\n",
        "        return out,lengths # TODO: Need to return 2 variables\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GFSLCBHCBk61"
      },
      "outputs": [],
      "source": [
        "model = Network().to(\"cuda\")\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "print(model)\n",
        "for i, data in enumerate(train_loader, 0):\n",
        "    \n",
        "    # Write a test code do perform a single forward pass and also compute the Levenshtein distance\n",
        "    # Make sure that you are able to get this right before going on to the actual training\n",
        "    # You may encounter a lot of shape errors\n",
        "    # Printing out the shapes will help in debugging\n",
        "    # Keep in mind that the Loss which you will use requires the input to be in a different format and the decoder expects it in a different format\n",
        "    # Make sure to read the corresponding docs about it\n",
        "\n",
        "    x, y, lx, ly = data\n",
        "    x = x.cuda()\n",
        "    y = y.cuda()\n",
        "    #y = y.transpose(0,1)\n",
        "    # y = y.reshape((y.shape[0],y.shape[1]))\n",
        "    out,h = model.forward(x,lx)\n",
        "    # print(out.shape)\n",
        "    # print(y.shape)\n",
        "    # lx = lx.cpu()\n",
        "    y = torch.tensor(y,dtype = torch.long)\n",
        "    out = out.reshape((-1,5))\n",
        "    y = y.reshape((-1))\n",
        "    loss = criterion(out,y)\n",
        "    # print(loss)\n",
        "    del x,y\n",
        "    torch.cuda.empty_cache()\n",
        "    #out - L , B  , C\n",
        "    #pdb.set_trace()\n",
        "\n",
        "    #break # one iteration is enough"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sy80BDiJdyFu"
      },
      "outputs": [],
      "source": [
        "def val(model, valid_loader, epoch):\n",
        "    model.eval()\n",
        "   \n",
        "    labels = [-1,0,1,2,3]\n",
        "    true_y_list = []\n",
        "    pred_y_list = []\n",
        "\n",
        "    for i, (x, y, lx, ly) in enumerate(val_loader):\n",
        "      x = x.cuda()\n",
        "      y = y.cuda()\n",
        "      # print(x.shape)\n",
        "      # print(y.shape)\n",
        "\n",
        "      with torch.no_grad():\n",
        "          out, length = model(x, lx)\n",
        "          \n",
        "          out = out.reshape((-1,5))\n",
        "          y = y.reshape((-1))\n",
        "          label_idx = torch.argmax(out, axis=1)\n",
        "          pred_label = label_idx\n",
        "          \n",
        "          pred_y_list.extend(pred_label.tolist())\n",
        "          true_y_list.extend(y.tolist())\n",
        "          eval_accuracy =  accuracy_score(true_y_list, pred_y_list)\n",
        "\n",
        "    print(f\"Epoch: {epoch}\\t EvaluateACC: {eval_accuracy}\\t\")\n",
        "    \n",
        "    val_metrics = {\"epoch\":epoch,\n",
        "             \"val_ACC\": eval_accuracy}\n",
        "\n",
        "    wandb.log(val_metrics)\n",
        "    \n",
        "    return eval_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KCk5lkL2Bwiz"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()\n",
        "epochs = 100\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Device: \", device)\n",
        "model = Network().to(device)\n",
        "print(model)\n",
        "\n",
        "\n",
        "model.train()\n",
        "# TODO: Write the model training code \n",
        "\n",
        "# You are free to write your own code for training or you can use the code from previous homeworks' starter notebooks\n",
        "# However, you will have to make modifications because of the following.\n",
        "# (1) The dataloader returns 4 items unlike 2 for hw2p2\n",
        "# (2) The model forward returns 2 outputs\n",
        "# (3) The loss may require transpose or permuting\n",
        "# scaler = torch.cuda.amp.GradScaler()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 15, gamma=0.7)\n",
        "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=(len(train_loader) * epochs))\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "wandb.init(\n",
        "        project=\"IDL_Project\",\n",
        "        name=\"1layer_LSTM_with_2CNN_kernel1\",\n",
        "        config={\"epochs\": 100,\n",
        "            \"batch_size\": 128,\n",
        "            \"lr\": 1e-3,\n",
        "            })\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    # Quality of life tip: leave=False and position=0 are needed to make tqdm usable in jupyter\n",
        "    batch_bar = tqdm(total=len(train_loader), dynamic_ncols=True, leave=False, position=0, desc='Train') \n",
        "\n",
        "    num_correct = 0\n",
        "    total_loss = 0\n",
        "\n",
        "    for i, data in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Don't be surprised - we just wrap these two lines to make it work for FP16\n",
        "        with torch.cuda.amp.autocast():  \n",
        "            x, y, lx, ly = data\n",
        "            x = x.cuda()\n",
        "            y = y.cuda()\n",
        "            #y = y.transpose(0,1)\n",
        "            y = y.reshape((y.shape[0],y.shape[1]))\n",
        "            out,h = model.forward(x,lx)\n",
        "            # lx = lx.cpu()\n",
        "            y = torch.tensor(y,dtype = torch.long)\n",
        "            out = out.reshape((-1,5))\n",
        "            y = y.reshape((-1))\n",
        "            loss = criterion(out,y)\n",
        "            del x,y\n",
        "\n",
        "        # Update # correct & loss as we go\n",
        "        #num_correct += int((torch.argmax(outputs, axis=1) == y).sum())\n",
        "        total_loss += float(loss)\n",
        "\n",
        "        # tqdm lets you add some details so you can monitor training as you train.\n",
        "        batch_bar.set_postfix(\n",
        "            #acc=\"{:.04f}%\".format(100 * num_correct / ((i + 1) * batch_size)),\n",
        "            loss=\"{:.04f}\".format(float(total_loss / (i + 1))),\n",
        "            #num_correct=num_correct,\n",
        "            lr=\"{:.04f}\".format(float(optimizer.param_groups[0]['lr'])))\n",
        "        \n",
        "        # Another couple things you need for FP16. \n",
        "        #scaler.scale(loss).backward() # This is a replacement for loss.backward()\n",
        "        #scaler.step(optimizer) # This is a replacement for optimizer.step()\n",
        "        #scaler.update() # This is something added just for FP16\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        #  We told scheduler T_max that we'd call step() (len(train_loader) * epochs) many times.\n",
        "\n",
        "        batch_bar.update() # Update tqdm bar\n",
        "\n",
        "    metrics = {\"epoch\":epoch,\n",
        "          \"train_loss\": total_loss / len(train_loader), \n",
        "          \"train_lr\": float(optimizer.param_groups[0]['lr'])}\n",
        "    \n",
        "    wandb.log(metrics)\n",
        "    # torch.save(model,'/content/drive/MyDrive/project.pkl')\n",
        "    batch_bar.close() # You need this to close the tqdm bar\n",
        "\n",
        "    # You can add validation per-epoch here if you would like\n",
        "    scheduler.step()\n",
        "    print(\"Epoch {}/{}: , Train Loss {:.04f}, Learning Rate {:.04f}\".format(\n",
        "        epoch + 1,\n",
        "        epochs,\n",
        "        float(total_loss / len(train_loader)),\n",
        "        float(optimizer.param_groups[0]['lr'])))\n",
        "  \n",
        "    val(model, val_data, epoch)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "mode = 'train'\n",
        "model_path = \"/content/drive/MyDrive/idl_project/trained_model\"\n",
        "model_name = \"LSTM_CNN\" + time.strftime(\"%Y_%m_%d_%H_%M_%S\", time.localtime())\n",
        "torch.save(model.state_dict(),model_path + model_name)\n",
        "print(\"save success \" + model_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODvY3aDyfb-k",
        "outputId": "85748394-9237-42cb-a5f6-55cbc174cb7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "save success LSTM_CNN2022_05_02_06_39_21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NsQ72dG-jnzA",
        "outputId": "7872b37a-d1f6-485b-b2ca-1dff63883aca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:55: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([40040])\n",
            "torch.Size([40040, 5])\n",
            "torch.Size([40040])\n",
            "tensor(74305, device='cuda:0')\n",
            "0.9935314685314686\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "labels = [-1,0,1,2,3]\n",
        "true_y_list = []\n",
        "pred_y_list = []\n",
        "\n",
        "for i, (x, y, lx, ly) in enumerate(test_loader):\n",
        "      x = x.cuda()\n",
        "      y = y.cuda()\n",
        "      # print(x.shape)\n",
        "      # print(y.shape)\n",
        "\n",
        "      with torch.no_grad():\n",
        "          out, length = model(x, lx)\n",
        "          \n",
        "          out = out.reshape((-1,5))\n",
        "          y = y.reshape((-1))\n",
        "          label_idx = torch.argmax(out, axis=1)\n",
        "          pred_label = label_idx\n",
        "          \n",
        "          pred_y_list.extend(pred_label.tolist())\n",
        "          true_y_list.extend(y.tolist())\n",
        "          eval_accuracy =  accuracy_score(true_y_list, pred_y_list)\n",
        "\n",
        "          print(pred_label.shape)\n",
        "          print(out.shape)\n",
        "          print(y.shape)\n",
        "          print(torch.argmax(out))\n",
        "          print(eval_accuracy)\n",
        "          "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r6uF7QlMmFE-"
      },
      "outputs": [],
      "source": [
        "dirn = str(ROOT_DIR) \n",
        "res = get_file_paths(dirn)\n",
        "paths = res[\"0cloth\"]\n",
        "# \"0cloth\" , \"1cloth \" , \"2cloth\" , \"3cloth\"\n",
        "dat0 = get_data(res[\"0cloth\"])\n",
        "dat1 = get_data(res[\"1cloth\"])\n",
        "dat2 = get_data(res[\"2cloth\"])\n",
        "dat3 = get_data(res[\"3cloth\"])\n",
        "\n",
        "path0 = res[\"0cloth\"]\n",
        "path1 = res[\"1cloth\"]\n",
        "path2 = res[\"2cloth\"]\n",
        "path3 = res[\"3cloth\"]\n",
        "\n",
        "# data = std_normalizer(data)\n",
        "\n",
        "data0 = ClothDataSet(path0, None)\n",
        "data1 = ClothDataSet(path1, None)\n",
        "data2 = ClothDataSet(path2, None)\n",
        "data3 = ClothDataSet(path3, None)\n",
        "\n",
        "test_loader0 = DataLoader(data0, batch_size=128,collate_fn=data0.collate_fn,\n",
        "                      shuffle=False, drop_last=True, num_workers=2)\n",
        "test_loader1 = DataLoader(data1, batch_size=128,collate_fn=data1.collate_fn,\n",
        "                      shuffle=False, drop_last=True, num_workers=2)\n",
        "test_loader2 = DataLoader(data2, batch_size=128,collate_fn=data2.collate_fn,\n",
        "                      shuffle=False, drop_last=True, num_workers=2)\n",
        "test_loader3 = DataLoader(data3, batch_size=128,collate_fn=data3.collate_fn,\n",
        "                      shuffle=False, drop_last=True, num_workers=2)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FkqVscd9wr20"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import plot_confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-_qBoeNVss0F",
        "outputId": "0b88ceb7-ae07-42b5-ccd5-cdb85d168a9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:55: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[    0     0     0     0     0]\n",
            " [    0 32586     7    35   471]\n",
            " [    0    10 10387     0     0]\n",
            " [    0    19     0 10959     0]\n",
            " [    0   372     0     0 10460]]\n",
            "torch.Size([76356])\n",
            "torch.Size([76356, 5])\n",
            "torch.Size([76356])\n",
            "tensor(119495, device='cuda:0')\n",
            "0.9856592802137356\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "labels = [-1,0,1,2,3]\n",
        "true_y_list = []\n",
        "pred_y_list = []\n",
        "matrix = []\n",
        "for i, (x, y, lx, ly) in enumerate(val_loader):\n",
        "      x = x.cuda()\n",
        "      y = y.cuda()\n",
        "      # print(x.shape)\n",
        "      # print(y.shape)\n",
        "\n",
        "      with torch.no_grad():\n",
        "          out, length = model(x, lx)\n",
        "          \n",
        "          out = out.reshape((-1,5))\n",
        "          y = y.reshape((-1))\n",
        "          label_idx = torch.argmax(out, axis=1)\n",
        "          pred_label = label_idx\n",
        "          \n",
        "          pred_y_list.extend(pred_label.tolist())\n",
        "          true_y_list.extend(y.tolist())\n",
        "          eval_accuracy =  accuracy_score(true_y_list, pred_y_list)\n",
        "          \n",
        "          matrix = confusion_matrix(true_y_list, pred_y_list, labels=[-1,0,1,2,3], sample_weight=None)\n",
        "          print(matrix)\n",
        "          print(pred_label.shape)\n",
        "          print(out.shape)\n",
        "          print(y.shape)\n",
        "          print(torch.argmax(out))\n",
        "          print(eval_accuracy)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "J6vXe2VdtYWv",
        "outputId": "e106dcbc-0604-4926-fc06-250dcdd9d4da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5, 5)\n",
            "[[32586     7    35   471]\n",
            " [   10 10387     0     0]\n",
            " [   19     0 10959     0]\n",
            " [  372     0     0 10460]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "seaborn.set()\n",
        "f,ax=plt.subplots()\n",
        "# y_true = [0,0,1,2,1,2,0,2,2,0,1,1]\n",
        "# y_pred = [1,0,1,2,1,0,0,2,2,0,1,1]\n",
        "# C2= confusion_matrix(y_true, y_pred, labels=[-1,0,1,2,3])\n",
        "print(matrix.shape) \n",
        "\n",
        "matrix_plot = matrix[1:, 1:]\n",
        "cm_normalized = matrix_plot.astype('float') / matrix_plot.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "seaborn.heatmap(cm_normalized,annot=True,ax=ax, cmap='Blues') \n",
        "\n",
        "ax.xaxis.set_ticklabels(['cloth0','cloth1','cloth2','cloth3'])\n",
        "ax.yaxis.set_ticklabels(['cloth0','cloth1','cloth2','cloth3'])\n",
        "ax.set_title('confusion matrix validation dataset') \n",
        "ax.set_xlabel('predict') \n",
        "ax.set_ylabel('true') \n",
        "\n",
        "print(matrix_plot)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "11785_Project_wc.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}